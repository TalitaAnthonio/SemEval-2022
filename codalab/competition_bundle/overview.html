<H3>Identifying Plausible Clarifications of Implicit and Underspecified Phrases in Instructional Texts</H3>

<p>
The goal of this shared task is to evaluate the ability of NLP systems to distinguish between plausible and implausible clarifications of an instruction. 
Such clarifications can be critical to ensure that instructions describe clearly enough what steps must be followed to achieve a specific goal. 
We set up this task as a cloze task, in which clarifications are presented as possible fillers and systems have to score how well each filler plausibly fits in a given context.
Cloze tasks have become a standard framework for evaluating various discourse-level phenomena in NLP. Some prominent examples include the narrative cloze test (Chambers and Jurafksy, 2008), the story cloze test (Mostafazadeh et al., 2016), and the LAMBADA word prediction task (Paperno et al., 2016). In these tasks, NLP systems are required to make a prediction about the filler of a cloze that is most likely to continue the discourse. However, it is not always clear whether exactly one likely filler exists or how plausible different fillers would be.
This task revolves around judging the plausibility of human-inserted and machine-generated fillers in naturally occurring contexts. 
Specifically, the contexts are instructional texts on everyday scenarios in which clarifications may have been necessary to eliminate possible misunderstandings. 
Clarifications were identified using revision histories in which it is possible to observe disambiguations of various semantic and pragmatic phenomena, including implicit, 
underspecified, and metonymic references, as well as implicit discourse relations and implicit quantifying modifiers.
</p>