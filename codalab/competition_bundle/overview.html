<h3>SemEval-2022 Task 7: Identifying Plausible Clarifications of Implicit and Underspecified Phrases in Instructional Texts</h3>
<div>
    <p>
        The goal of this shared task is to evaluate the ability of NLP systems to distinguish between <b>plausible and implausible clarifications</b> of an instruction.
    </p>
    <p>
        A clarification is a revision that makes an text easier to understand and avoids possible misunderstandings.
        For example, a clarification can make implicit elements in a text explicit, resolve ambiguities or replace underspecified phrases by a more precise expression.
        These modifications are particularly important in instructional texts like how-to guides, manuals or recipes to ensure that they describe clearly enough what steps must be followed to achieve a specific goal.
    </p>
    <p>
        Therefore, the task of determining how plausible a clarification is in a given context can be useful to
        <ul>
            <li>find ways to improve instructional texts</li>
            <li>evaluate to what extent current NLP systems are able to handle implicit, ambiguous and underspecified language</li>
            <li>go beyond the surface form of a text and take multiple plausible interpretations into account</li>
        </ul>
    </p>
</div>

<h4>Task Set-Up</h4>
<div>
    <p>Systems have to predict how well each filler fits in a given context.</p>
    <p>There are two subtasks: a classification task and a ranking task.</p>
</div>

<h5>Subtask A: Multi-Class Classification</h5>
<div>
    <p>The goal is to predict a class label (implausible/not-sure/plausible) given the clarification and its context.</p>
    <p>Submitted systems will be evaluated using the accuracy score.</p>
</div>

<h5>Subtask B: Ranking</h5>
<div>
    <p>The goal is to predict the plausibility score on a scale from 1 to 5 given the clarification and its context.</p>
    <p>
        All clarifications in the evaluation set are ordered by the predicted plausibility to create a ranking.
        This ranking is then compared to the ranking that was established by the gold plausibility scores.
        The evaluation metric for this comparison is Spearman's rank correlation coefficient.
    </p>
</div>

<h4>Want to Get in Touch? Need a Clarification?</h4>
<div>
    <p>There is no formal registration for the task yet. Anyone interested can join our Google group <a href="https://groups.google.com/g/semeval2022-task7/">here</a>.</p>
    <p>For further information and to access the trial and training data, visit <a href="https://clarificationtask.github.io">this website</a>.</p>
</div>